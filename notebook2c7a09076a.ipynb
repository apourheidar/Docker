{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nquestion_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\ncontext = r\"\"\"\nExtractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\nquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\na model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n\"\"\"\n\nresult = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T09:07:45.944543Z","iopub.execute_input":"2025-02-05T09:07:45.944994Z","iopub.status.idle":"2025-02-05T09:08:21.668719Z","shell.execute_reply.started":"2025-02-05T09:07:45.944964Z","shell.execute_reply":"2025-02-05T09:08:21.667573Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d30c0f8afe4a7687c035d8df764730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7320e1443e9c45689018ad27aca85c94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c7101d248d43a0a0a4d58080535c02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78053ce1880d4f588f4024aa2495dabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2503cce9a09c4ca2802e7bfda5ad676d"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"Answer: 'SQuAD dataset', score: 0.4704, start: 151, end: 164\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nanswer_start_index = torch.argmax(outputs.start_logits)\nanswer_end_index = torch.argmax(outputs.end_logits)\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T09:10:16.972699Z","iopub.execute_input":"2025-02-05T09:10:16.973149Z","iopub.status.idle":"2025-02-05T09:10:17.445249Z","shell.execute_reply.started":"2025-02-05T09:10:16.973119Z","shell.execute_reply":"2025-02-05T09:10:17.444169Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'a nice puppet'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T09:11:30.731968Z","iopub.execute_input":"2025-02-05T09:11:30.732431Z","iopub.status.idle":"2025-02-05T09:11:33.720794Z","shell.execute_reply.started":"2025-02-05T09:11:30.732397Z","shell.execute_reply":"2025-02-05T09:11:33.719796Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n\nAll the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'a nice puppet'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import pipeline\nquestion_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\ncontext = r\"\"\"\nAlice is sitting on the bench. Bob is sitting next to her.\n\"\"\"\n\nresult = question_answerer(question=\"Who is the CEO?\", context=context)\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T09:41:25.496652Z","iopub.execute_input":"2025-02-05T09:41:25.497075Z","iopub.status.idle":"2025-02-05T09:41:25.856103Z","shell.execute_reply.started":"2025-02-05T09:41:25.497047Z","shell.execute_reply":"2025-02-05T09:41:25.854847Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"Answer: 'Bob', score: 0.4183, start: 32, end: 35\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import transformers\nimport keras\nimport tensorflow\nprint(transformers.__version__)\nprint(keras.__version__)\nprint(tensorflow.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T10:13:47.799184Z","iopub.execute_input":"2025-02-05T10:13:47.799720Z","iopub.status.idle":"2025-02-05T10:13:47.807135Z","shell.execute_reply.started":"2025-02-05T10:13:47.799686Z","shell.execute_reply":"2025-02-05T10:13:47.805883Z"}},"outputs":[{"name":"stdout","text":"4.47.0\n3.5.0\n2.17.1\n","output_type":"stream"}],"execution_count":18}]}